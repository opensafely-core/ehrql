#!/usr/bin/env python3
import configparser
import os
import subprocess
import sys
import time
from urllib.parse import urljoin

import click
import requests

PROFILE = os.environ.get("DATABRICKS_PROFILE", "DEFAULT")
CLUSTERS = None
ORGID = None
USERNAME = None
PASSWORD = None
HOST = None
COMMANDS = {}
OUTPUT = True

VENV = os.environ["VIRTUAL_ENV"]
DATABRICKS_EXEC = f"{VENV}/bin/databricks"
PYTHON = f"{VENV}/bin/python"


def run(cmd, check=True, text=True, **kwargs):
    echo(" ".join(cmd).replace(PASSWORD, "*****"))
    return subprocess.run(cmd, check=check, text=text, **kwargs)


def databricks(cmd, *args, **kwargs):
    cmd = [DATABRICKS_EXEC, "--profile", PROFILE] + cmd
    return run(cmd, *args, **kwargs)


def is_community_edition():
    return "community.cloud.databricks.com" in HOST


def echo(*args, **kwargs):
    if OUTPUT:
        click.echo(*args, **kwargs)


class Cluster:
    def __init__(self, data):
        self.data = data

    @property
    def cluster_id(self):
        return self.data["cluster_id"]

    @property
    def state(self):
        return self.data["state"]

    @property
    def url(self):
        host_with_creds = HOST.replace("https://", f"https://{USERNAME}:{PASSWORD}@")
        return f"{host_with_creds}/default?http_path=sql/protocolv1/o/{ORGID}/{self.cluster_id}"

    def __str__(self):
        return f"{self.cluster_id}: {self.state}"


def get_auth():
    global USERNAME, PASSWORD, HOST
    path = os.path.expanduser("~/.databrickscfg")
    if not os.path.exists(path):
        sys.exit("databricks cli has not been configured. Run:\ndatabricks configure")
    parser = configparser.ConfigParser()
    parser.read(path)
    profile = parser[PROFILE]
    USERNAME = profile["username"]
    PASSWORD = profile["password"]
    HOST = profile["host"]


def get_clusters():
    # we hit the API directly here, as we need access to the headers.
    global CLUSTERS, ORGID

    resp = requests.get(
        urljoin(HOST, "/api/2.0/clusters/list"),
        auth=(USERNAME, PASSWORD),
    )
    resp.raise_for_status()

    CLUSTERS = {c["cluster_name"]: Cluster(c) for c in resp.json().get("clusters", {})}
    ORGID = resp.headers["X-Databricks-Org-Id"]


def _wait(name, timeout=90):
    echo(CLUSTERS[name])
    start = time.time()
    # We need to wait if the state is TERMINATED, as the first try after starting may not
    # register it as PENDING yet
    while CLUSTERS[name].state in ["PENDING", "TERMINATED"]:
        if time.time() - start > timeout:
            sys.exit(f"timed out after {timeout}s waiting for cluster {name}")
        time.sleep(10)
        get_clusters()
        echo(CLUSTERS[name])


@click.group()
def cli():
    """Command for managing databricks cluster.

    The default cluster name is 'opensafely-test', but can be overridden with
    --name option or the DATABRICKS_CLUSTER env var.

    It will ensure the cluster is running, but will not create it if it doesn't already exist.

    By default, it uses the DEFAULT profile of your databricks cli config, but
    this can be altered with the DATABRICKS_PROFILE env var.
    """
    pass


# common option for many commands
name_option = click.option("--name", default="opensafely-test", help="name of cluster")


@cli.command()
@name_option
@click.option(
    "--wait/--no-wait",
    default=False,
    help="wait for cluster to be available before exiting",
)
@click.option("--timeout", default=90, help="timeout to wait if --wait is passed")
@click.option(
    "--output-url/--no-output-url", default=False, help="only output the connection url"
)
def start(name, wait, output_url, timeout):
    """Ensure cluster is running."""
    global OUTPUT

    if output_url:
        OUTPUT = False

    c = CLUSTERS.get(name)

    # it exists
    if c:
        # it's running or starting up
        if c.state in ("PENDING", "RUNNING"):
            if output_url:
                click.echo(c.url)
            else:
                echo(f"{HOST} {c}")
            return
        else:
            # cluster exists, but is not running or starting up
            click.echo("Starting cluster")
            databricks(["clusters", "start", "--cluster-id", c.cluster_id])
    else:
        click.echo(
            f"No {name} cluster found.\n"
            f"You will need to create one called '{name}' via the web UI."
        )
        sys.exit(1)

    get_clusters()

    if wait:
        _wait(name, timeout)

    if output_url:
        click.echo(CLUSTERS[name].url)
    else:
        echo(CLUSTERS[name])


@cli.command()
@name_option
def wait(name):
    """Wait until cluster is available."""
    _wait(name)


@cli.command()
@name_option
def url(name):
    """Print connection string."""
    c = CLUSTERS.get(name)
    if c:
        click.echo(c.url)
    else:
        click.echo(f"No cluster named {name}")


@cli.command()
@name_option
def test(name):
    """Test cluster up and working."""
    _wait(name)
    c = CLUSTERS[name]
    run(
        [
            PYTHON,
            "-m",
            "databuilder",
            "test-connection",
            "-b",
            "databuilder.backends.databricks.DatabricksBackend",
            "-u",
            c.url,
        ]
    )


@cli.command()
@name_option
def status(name):
    """Print cluster status."""
    if name in CLUSTERS:
        click.echo(CLUSTERS[name])
    else:
        click.echo(f"Cluster {name} not found")


@cli.command()
def cleanup():
    """Remove all data on the cluster."""
    if is_community_edition():
        databricks(
            [
                "workspace",
                "import",
                "-l",
                "python",
                "-o",
                "scripts/dbx-cleanup.py",
                "/cleanup",
            ]
        )
        click.echo(
            "The cleanup notebook has been uploaded to your Databricks Workspace.\n"
            "Unfortunately, it cannot be executed remotely in Community Edition.\n"
            "Log in to https://community.cloud.databricks.com and select Workspace -> cleanup and run it manually."
        )
    else:
        ps = databricks(["fs", "ls", "dbfs:/user/hive/warehouse/"], capture_output=True)
        for db in ps.stdout.split():
            databricks(["fs", "rm", "-r", f"dbfs:/user/hive/warehouse/{db}"])


if __name__ == "__main__":
    get_auth()
    get_clusters()
    cli()
